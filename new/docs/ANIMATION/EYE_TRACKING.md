# Eye Tracking & Gaze Following

## Overview

Real-time eye tracking using face detection camera to make avatar eyes follow user's position.

```
Camera frame
    ‚Üì
Face detection
    ‚Üì
Get user face position
    ‚Üì
Calculate gaze vector
    ‚Üì
Animate eyes to follow user
```

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   BK7258 Camera (front-facing)     ‚îÇ
‚îÇ   ‚îú‚îÄ 320x240 resolution            ‚îÇ
‚îÇ   ‚îú‚îÄ 15 FPS capture                ‚îÇ
‚îÇ   ‚îî‚îÄ ~66ms per frame               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ Face Detection ‚îÇ
        ‚îÇ (MTCNN/YOLOv8) ‚îÇ
        ‚îÇ ~20-30ms       ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ Gaze Calculator‚îÇ
        ‚îÇ ~5-10ms        ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Eye Animation Commands‚îÇ
    ‚îÇ Send to device        ‚îÇ
    ‚îÇ Real-time update      ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Device Renders Eyes   ‚îÇ
    ‚îÇ Following user        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Implementation

```python
# eye_tracking.py
import cv2
import numpy as np
from typing import Tuple, Optional, Dict
import asyncio
import time
from dataclasses import dataclass

@dataclass
class FaceDetection:
    """Face detection result"""
    x: int          # Face left
    y: int          # Face top
    width: int      # Face width
    height: int     # Face height
    confidence: float

@dataclass
class GazeVector:
    """Gaze vector (where user is looking relative to camera)"""
    x: float        # -1.0 (left) to 1.0 (right)
    y: float        # -1.0 (down) to 1.0 (up)
    confidence: float

class EyeTracker:
    """Real-time eye tracking using face detection"""
    
    def __init__(self, camera_id: int = 0):
        self.camera_id = camera_id
        self.cap = cv2.VideoCapture(camera_id)
        
        # Camera settings
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 320)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 240)
        self.cap.set(cv2.CAP_PROP_FPS, 15)
        
        # Face detector (lightweight for real-time)
        try:
            # Try YOLOv8n (fastest)
            from ultralytics import YOLO
            self.detector = YOLO("yolov8n-face.pt")
            self.detector_type = "yolov8"
        except:
            # Fallback to OpenCV Haar Cascade
            self.detector = cv2.CascadeClassifier(
                cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            )
            self.detector_type = "haar"
        
        # Smoothing filter for gaze
        self.gaze_history = []
        self.history_size = 5
        
        print("‚úì Eye tracker initialized")
    
    async def get_gaze(self) -> Optional[GazeVector]:
        """Get current gaze vector (where user is looking)"""
        
        ret, frame = self.cap.read()
        
        if not ret:
            return None
        
        # Detect face
        face = self._detect_face(frame)
        
        if face is None:
            return None
        
        # Calculate gaze from face position
        gaze = self._calculate_gaze(frame, face)
        
        # Smooth with history
        gaze = self._smooth_gaze(gaze)
        
        return gaze
    
    def _detect_face(self, frame) -> Optional[FaceDetection]:
        """Detect face in frame"""
        
        if self.detector_type == "yolov8":
            results = self.detector(frame, conf=0.5)
            
            if len(results) > 0 and len(results[0].boxes) > 0:
                box = results[0].boxes[0]
                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)
                conf = float(box.conf[0])
                
                return FaceDetection(
                    x=x1,
                    y=y1,
                    width=x2 - x1,
                    height=y2 - y1,
                    confidence=conf
                )
        
        else:  # Haar Cascade
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            faces = self.detector.detectMultiScale(
                gray,
                scaleFactor=1.1,
                minNeighbors=5,
                minSize=(30, 30)
            )
            
            if len(faces) > 0:
                x, y, w, h = faces[0]
                return FaceDetection(
                    x=x,
                    y=y,
                    width=w,
                    height=h,
                    confidence=0.8
                )
        
        return None
    
    def _calculate_gaze(self, frame, face: FaceDetection) -> GazeVector:
        """Calculate gaze vector from face position"""
        
        h, w = frame.shape[:2]
        
        # Face center in image coordinates
        face_center_x = face.x + face.width / 2
        face_center_y = face.y + face.height / 2
        
        # Image center
        img_center_x = w / 2
        img_center_y = h / 2
        
        # Deviation from image center (normalized to -1, 1)
        gaze_x = (face_center_x - img_center_x) / (img_center_x)
        gaze_y = (img_center_y - face_center_y) / (img_center_y)
        
        # Clamp to reasonable range
        gaze_x = np.clip(gaze_x, -1.0, 1.0)
        gaze_y = np.clip(gaze_y, -1.0, 1.0)
        
        return GazeVector(
            x=float(gaze_x),
            y=float(gaze_y),
            confidence=face.confidence
        )
    
    def _smooth_gaze(self, gaze: GazeVector) -> GazeVector:
        """Smooth gaze with moving average"""
        
        self.gaze_history.append(gaze)
        
        if len(self.gaze_history) > self.history_size:
            self.gaze_history.pop(0)
        
        # Average
        avg_x = np.mean([g.x for g in self.gaze_history])
        avg_y = np.mean([g.y for g in self.gaze_history])
        avg_conf = np.mean([g.confidence for g in self.gaze_history])
        
        return GazeVector(
            x=float(avg_x),
            y=float(avg_y),
            confidence=float(avg_conf)
        )
    
    def close(self):
        """Release camera"""
        self.cap.release()
```

## Integration with Avatar Eyes

```python
# eye_gaze_animator.py
import asyncio
from eye_tracking import EyeTracker, GazeVector

class EyeGazeAnimator:
    """Animate avatar eyes to follow user gaze"""
    
    def __init__(self, device_connection):
        self.tracker = EyeTracker()
        self.device = device_connection
        self.running = False
        self.update_rate = 15  # Hz
    
    async def start_tracking(self):
        """Start real-time eye tracking loop"""
        
        self.running = True
        
        while self.running:
            try:
                # Get user gaze
                gaze = await self.tracker.get_gaze()
                
                if gaze is None or gaze.confidence < 0.5:
                    # No face detected, use default gaze
                    await self._send_neutral_gaze()
                else:
                    # Convert gaze to eye animation
                    eye_commands = self._gaze_to_eye_animation(gaze)
                    
                    # Send to device
                    await self.device.send_eye_commands(eye_commands)
                
                # Update at specified rate
                await asyncio.sleep(1.0 / self.update_rate)
            
            except Exception as e:
                print(f"Eye tracking error: {e}")
                await asyncio.sleep(0.1)
    
    def _gaze_to_eye_animation(self, gaze: GazeVector) -> Dict:
        """Convert gaze vector to eye animation commands"""
        
        # Map gaze to eye position (-1 to 1 becomes -0.3 to 0.3 for safe range)
        eye_x = gaze.x * 0.4
        eye_y = gaze.y * 0.4
        
        return {
            "type": "gaze_follow",
            "eye_x": eye_x,
            "eye_y": eye_y,
            "duration_ms": 50,
            "smooth": True,
            "confidence": gaze.confidence
        }
    
    async def _send_neutral_gaze(self):
        """Send neutral gaze (looking forward)"""
        
        await self.device.send_eye_commands({
            "type": "gaze_idle",
            "eye_x": 0.0,
            "eye_y": 0.0,
            "duration_ms": 50
        })
    
    def stop_tracking(self):
        """Stop eye tracking"""
        
        self.running = False
        self.tracker.close()
```

## WebSocket Integration

```python
# ws_eye_tracking.py
from fastapi import FastAPI, WebSocket
import asyncio
import json

app = FastAPI()

class EyeTrackingManager:
    """Manage eye tracking for connected devices"""
    
    def __init__(self):
        self.devices = {}
        self.trackers = {}
    
    async def start_tracking_for_device(self, device_id: str, websocket: WebSocket):
        """Start eye tracking for device"""
        
        if device_id in self.trackers:
            print(f"Tracker already running for {device_id}")
            return
        
        from eye_tracking import EyeTracker
        
        tracker = EyeTracker()
        self.trackers[device_id] = tracker
        self.devices[device_id] = websocket
        
        print(f"Started eye tracking for {device_id}")
        
        try:
            while True:
                # Get gaze
                gaze = await tracker.get_gaze()
                
                if gaze is None:
                    continue
                
                # Send to device via WebSocket
                await websocket.send_json({
                    "type": "gaze_update",
                    "eye_x": gaze.x * 0.4,
                    "eye_y": gaze.y * 0.4,
                    "confidence": gaze.confidence
                })
                
                await asyncio.sleep(1.0 / 15)  # 15 FPS
        
        except Exception as e:
            print(f"Eye tracking error: {e}")
        
        finally:
            tracker.close()
            del self.trackers[device_id]
            del self.devices[device_id]

eye_tracking_manager = EyeTrackingManager()

@app.websocket("/ws/eye-tracking/{device_id}")
async def eye_tracking_websocket(websocket: WebSocket, device_id: str):
    """WebSocket endpoint for eye tracking"""
    
    await websocket.accept()
    
    # Start tracking in background
    tracking_task = asyncio.create_task(
        eye_tracking_manager.start_tracking_for_device(device_id, websocket)
    )
    
    try:
        # Keep connection alive
        while True:
            await websocket.receive_text()
    
    except Exception as e:
        print(f"Error: {e}")
    
    finally:
        tracking_task.cancel()
        await websocket.close()
```

## Device-Side Rendering

```c
// device_eye_tracking.c
#include "display_driver.h"
#include "animation.h"

typedef struct {
    float eye_x;        // -0.3 to 0.3
    float eye_y;        // -0.3 to 0.3
    float confidence;
} GazeCommand;

// Current eye position
static float current_eye_x = 0.0;
static float current_eye_y = 0.0;

void update_gaze(GazeCommand cmd) {
    // Smooth interpolation
    float alpha = 0.2;  // Smoothing factor
    
    current_eye_x = current_eye_x * (1 - alpha) + cmd.eye_x * alpha;
    current_eye_y = current_eye_y * (1 - alpha) + cmd.eye_y * alpha;
}

void render_eyes_with_gaze(float emotion_blend, int mouth_shape) {
    // Eye position on display (240x240)
    int eye_spacing = 40;
    int left_eye_x = 80;
    int right_eye_x = 160;
    int eyes_y = 100;
    
    // Adjust for gaze
    int left_eye_gaze_x = left_eye_x + (int)(current_eye_x * 10);
    int left_eye_gaze_y = eyes_y + (int)(current_eye_y * 8);
    
    int right_eye_gaze_x = right_eye_x + (int)(current_eye_x * 10);
    int right_eye_gaze_y = eyes_y + (int)(current_eye_y * 8);
    
    // Draw eyes
    draw_eye(
        left_eye_gaze_x, left_eye_gaze_y,
        20,  // radius
        emotion_blend
    );
    
    draw_eye(
        right_eye_gaze_x, right_eye_gaze_y,
        20,
        emotion_blend
    );
}

void draw_eye(int x, int y, int radius, float emotion) {
    // Draw eye circle
    display_draw_circle(x, y, radius, DISPLAY_WHITE);
    
    // Draw pupil offset by current gaze
    int pupil_x = x + (int)(current_eye_x * 8);
    int pupil_y = y + (int)(current_eye_y * 6);
    
    int pupil_color = emotion_to_pupil_color(emotion);
    display_draw_circle(pupil_x, pupil_y, 8, pupil_color);
    
    // Draw shine (reflection)
    display_draw_circle(pupil_x - 2, pupil_y - 2, 2, DISPLAY_WHITE);
}

int emotion_to_pupil_color(float emotion_blend) {
    // Map emotion to pupil color
    // 0.0 = neutral (black), 1.0 = happy (warm)
    
    int r = (int)(emotion_blend * 50);
    int g = (int)(emotion_blend * 30);
    int b = (int)(emotion_blend * 10);
    
    return DISPLAY_RGB(r, g, b);
}
```

## Filtering & Smoothing

```python
# gaze_filter.py
import numpy as np
from collections import deque

class GazeFilter:
    """Advanced gaze filtering"""
    
    def __init__(self, window_size: int = 7):
        self.window = deque(maxlen=window_size)
        self.kalman_x = KalmanFilter(0.1, 0.1)
        self.kalman_y = KalmanFilter(0.1, 0.1)
    
    def filter_gaze(self, raw_gaze) -> tuple:
        """Apply Kalman filter + moving average"""
        
        # Kalman filter
        filtered_x = self.kalman_x.update(raw_gaze.x)
        filtered_y = self.kalman_y.update(raw_gaze.y)
        
        # Moving average
        self.window.append((filtered_x, filtered_y))
        
        avg_x = np.mean([g[0] for g in self.window])
        avg_y = np.mean([g[1] for g in self.window])
        
        return avg_x, avg_y

class KalmanFilter:
    """1D Kalman filter"""
    
    def __init__(self, process_variance, measurement_variance):
        self.process_variance = process_variance
        self.measurement_variance = measurement_variance
        self.value = 0.0
        self.estimate_error = 1.0
    
    def update(self, measurement):
        # Prediction
        self.estimate_error = self.estimate_error + self.process_variance
        
        # Update
        gain = self.estimate_error / (self.estimate_error + self.measurement_variance)
        self.value = self.value + gain * (measurement - self.value)
        self.estimate_error = (1 - gain) * self.estimate_error
        
        return self.value
```

## Performance Monitoring

```python
# eye_tracking_monitor.py
import time
from collections import deque

class EyeTrackingMonitor:
    """Monitor eye tracking performance"""
    
    def __init__(self, window_size=100):
        self.latencies = deque(maxlen=window_size)
        self.detection_rates = deque(maxlen=window_size)
        self.gaze_values = deque(maxlen=window_size)
    
    def record_frame(self, latency_ms: float, detected: bool, gaze):
        """Record frame metrics"""
        
        self.latencies.append(latency_ms)
        self.detection_rates.append(1.0 if detected else 0.0)
        
        if detected and gaze:
            self.gaze_values.append(gaze)
    
    def get_stats(self):
        """Get performance statistics"""
        
        import statistics
        
        return {
            "avg_latency_ms": statistics.mean(self.latencies) if self.latencies else 0,
            "p95_latency_ms": sorted(self.latencies)[int(len(self.latencies) * 0.95)] if len(self.latencies) > 1 else 0,
            "detection_rate": statistics.mean(self.detection_rates) if self.detection_rates else 0,
            "fps": 1000 / (statistics.mean(self.latencies) if self.latencies else 66)
        }
```

## Testing

```python
# test_eye_tracking.py
import pytest
import asyncio

async def test_eye_tracker():
    """Test eye tracking initialization"""
    
    from eye_tracking import EyeTracker
    
    tracker = EyeTracker()
    
    # Get a few frames
    for _ in range(5):
        gaze = await tracker.get_gaze()
        # May be None if no face detected
    
    tracker.close()

async def test_gaze_smoothing():
    """Test gaze smoothing"""
    
    from eye_tracking import EyeTracker, GazeVector
    
    tracker = EyeTracker()
    
    # Simulate raw gaze values
    test_gazes = [
        GazeVector(0.1, 0.2, 0.9),
        GazeVector(0.15, 0.18, 0.9),
        GazeVector(0.12, 0.21, 0.9),
    ]
    
    smoothed = []
    for gaze in test_gazes:
        smoothed.append(tracker._smooth_gaze(gaze))
    
    # Check smoothing works
    assert len(smoothed) == 3
    
    tracker.close()
```

## Summary

- **Real-time face detection** - 320x240 @ 15 FPS
- **Gaze calculation** - 20-30ms per frame
- **Smooth interpolation** - Kalman filter + moving average
- **WebSocket streaming** - Live gaze updates
- **Device rendering** - Eye following animation
- **Emotion-aware** - Pupil color changes with emotion
- **Performance monitoring** - FPS, latency, detection rate
- **Production-ready** - Error handling + fallbacks

Perfect –¥–ª—è interactive eye contact! üëÄ
